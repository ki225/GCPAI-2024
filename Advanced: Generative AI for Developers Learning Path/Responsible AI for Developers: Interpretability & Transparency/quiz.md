1. You're working on an image classification model for identifying different types of clouds. During testing, you notice some strange results. The model seems to be focusing on irrelevant areas of the images (like background objects) instead of the actual cloud features. To understand why your model is behaving this way, which of the following tools or techniques would be the most helpful?
- [ ] ACE (Automatic Concept-based Explanation)
- [ ] Permutation Feature Importance
- [x] XRAI (eXplainable Region-based Artificial Intelligence)
- [ ] TCAV (Testing with Concept Activation Vectors)

>  - TCAV focuses on associating predictions with broader human-understandable concepts, not pinpointing specific pixel-level influences relevant to this scenario.
>  -  XRAI is a technique designed to highlight the specific pixels or regions of an image that are most influential in the model's decision.
 
2. Your manager is a strong advocate for responsible AI development. During a project review, they emphasize the importance of transparency and documentation around the dataset you're using and the resulting AI model. They want to know what steps you're taking to ensure this. Which of the following tools or techniques are you using to provide a clear understanding of your dataset and model?
- [ ] SHAP library.
- [ ] Vertex Explainable AI
- [x] Data card, Model card.
- [ ] Learning Interpretability Tool (LIT).

3. As an AI engineering team manager, you're giving a presentation to explain why interpretability and transparency in AI development is important for engineers. Which of the following statements would be best to include in your slides?
- [ ] Shorten project lead time.
- [x] Understand model behaviors
- [ ] Compliance.
- [ ] Reduce project cost.
