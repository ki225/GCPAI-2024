
1. **生成式 AI 興起**：
   - 近期的生成式 AI 進展引起廣泛關注，包括新的 Vertex AI 功能，例如 Gen AI Studio、Model Garden 和 Gen AI API。
   - 目標是幫助理解使生成式 AI 成為可能的基礎概念。

2. **語言模型的演變**：
   - 語言建模隨著時間演變，過去十年的突破包括使用神經網絡來表示文本，如 2013 年的 Word2Vec 和 N-grams。
   - 2014 年的序列到序列模型（如 RNN 和 LSTM）提升了 NLP 任務的性能（翻譯和文本分類）。
   - 2015 年引入的注意力機制和基於此的模型（如 Transformers 和 BERT 模型）引起了廣泛興奮。

3. **Transformers 模型**：
   - 基於 2017 年的論文《Attention Is All You Need》，Transformers 改變了單詞表示方式，能夠考慮上下文。
   - Transformers 是一種編碼器-解碼器模型，能夠同時處理大量數據，且具有優越的機器翻譯性能。

4. **編碼器和解碼器結構**：
   - 編碼器由多層相同結構的編碼器堆疊而成，並包含自注意力層和前饋層。
   - 解碼器也包含自注意力和前饋層，並有一個編碼器-解碼器注意力層，幫助解碼器關注輸入句子的相關部分。

5. **自注意力機制**：
   - 在自注意力層中，輸入嵌入被分解為查詢、鍵和值向量，並使用訓練期間學到的權重計算。
   - 所有計算以矩陣運算形式並行進行，最終產生自注意力層的輸出。

6. **BERT 模型**：
   - BERT 是由 Google 開發的雙向編碼器表示模型，於 2018 年推出，並已成為 Google 搜索的核心技術之一。
   - BERT 可處理長輸入上下文，並在維基百科和書籍語料庫上進行訓練，具有多任務目標。

7. **BERT 的訓練任務**：
   - 包括「掩蔽語言模型」和「下一句預測」任務，這使 BERT 能夠學習句子之間的關係並進行二元分類。

8. **嵌入層**：
   - BERT 的輸入需要三種嵌入：標記嵌入、段嵌入和位置嵌入，以區分輸入對的兩個句子並學習單詞順序。

9. **應用範圍**：
   - 雖然 BERT 專注於掩蔽語言建模和單句分類，但也可用於各種 NLP 任務，如單句分類和句對分類等。
